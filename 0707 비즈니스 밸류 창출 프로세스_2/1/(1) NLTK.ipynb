{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK 기본 사용법\n",
    "* pip install nltk를 통해 설치\n",
    "* 특정 모듈을 다운로드하기 위해서는 nltk.download()를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')  \n",
    "# nltk.download('maxent_treebank_pos_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nltk에서 제공되는 gutenberg data read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# 저장되어 있는 데이터 로드 및 파일 제목 확인\n",
    "gutenberg_files = gutenberg.fileids()\n",
    "gutenberg_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 특정 텍스트 확인\n",
    "gutenberg_doc = gutenberg.open('austen-emma.txt').read()\n",
    "print(gutenberg_doc[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenize - 띄어쓰기 기준으로 단어를 분리하여 list 형태로 저장\n",
    "\n",
    "sentence = \"\"\"At eight o'clock on Thursday morning ... Arthur didn't feel very good.\"\"\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PoS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pos tagging - token 단위로 Pos를 추가하여 tuple - list 형태로 저장\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged\n",
    "for word in tagged:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in tagged: \n",
    "    if word[1][0] == 'N':\n",
    "        print(word[0].lower()+'/'+word[1])\n",
    "\n",
    "# 같은 표현 = list comprehension\n",
    "tagged_word = [word[0].lower()+'/'+word[1] for word in tagged if word[1][0] == 'N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 결과 확인\n",
    "tagged_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alphabetical list of part-of-speech tags used in the Penn Treebank Project:\n",
    "https://www.cis.upenn.edu/~treebank/\n",
    "\n",
    "```CC Coordinating conjunction\n",
    "CD Cardinal number\n",
    "DT Determiner\n",
    "EX Existential there\n",
    "FW Foreign word\n",
    "IN Preposition or subordinating conjunction\n",
    "JJ Adjective\n",
    "JJR Adjective, comparative\n",
    "JJS Adjective, superlative\n",
    "LS List item marker\n",
    "MD Modal\n",
    "NN Noun, singular or mass\n",
    "NNS Noun, plural\n",
    "NNP Proper noun, singular\n",
    "NNPS Proper noun, plural\n",
    "PDT Predeterminer\n",
    "POS Possessive ending\n",
    "PRP Personal pronoun\n",
    "PRP$ Possessive pronoun\n",
    "RB Adverb\n",
    "RBR Adverb, comparative\n",
    "RBS Adverb, superlative\n",
    "RP Particle\n",
    "SYM Symbol\n",
    "TO to\n",
    "UH Interjection\n",
    "VB Verb, base form\n",
    "VBD Verb, past tense\n",
    "VBG Verb, gerund or present participle\n",
    "VBN Verb, past participle\n",
    "VBP Verb, non­3rd person singular present\n",
    "VBZ Verb, 3rd person singular present\n",
    "WDT Wh­determiner\n",
    "WP Wh­pronoun\n",
    "WP$ Possessive wh­pronoun\n",
    "WRB Wh­adverb```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이전에 불러온 gutenberg_doc 데이터를 가지고 tokenize + Pos tag를 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gutenberg_tokens = nltk.word_tokenize(gutenberg_doc)\n",
    "gutenberg_tagged = nltk.pos_tag(gutenberg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 시간을 체크하고 싶다면 time library를 사용해보자\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "## 실행 코드\n",
    "gutenberg_tokens = nltk.word_tokenize(gutenberg_doc)\n",
    "gutenberg_tagged = nltk.pos_tag(gutenberg_tokens)\n",
    "\n",
    "print(\"Processed time = \",(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gutenberg_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gutenberg_tagged[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 1 : Gutenberg file에서 다른 텍스트 데이터를 읽고 tokenize + PoS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gutenberg_doc = gutenberg.open('austen-sense.txt').read()\n",
    "gutenberg_tokens = nltk.word_tokenize(gutenberg_doc)\n",
    "gutenberg_tagged = nltk.pos_tag(gutenberg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gutenberg_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gutenberg_tagged[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming (or lemmatizing)\n",
    "단어의 어근을 추출하기 위해 stemming!\n",
    "\n",
    "Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications.\n",
    "\n",
    "The word \"meeting\" can be either the base form of a noun or a form of a verb (\"to meet\") depending on the context, e.g., \"in our last meeting\" or \"We are meeting again tomorrow\". Unlike stemming, lemmatisation can in principle select the appropriate lemma depending on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "gutenberg_lemma = []\n",
    "\n",
    "# 분리한 token에 대하여 nltk lemmatizing 하고 그 결과를 lemma list에 추가\n",
    "for token in gutenberg_tokens:\n",
    "    gutenberg_lemma.append(lemma.lemmatize(token))\n",
    "\n",
    "gutenberg_lemma[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenizing -> lemmatizing -> PoS tagging\n",
    "\n",
    "gutenberg_lemma_tagged = nltk.pos_tag(gutenberg_lemma)\n",
    "gutenberg_lemma_tagged[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stemming\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "gutenberg_stemmed = []\n",
    "for token in gutenberg_tokens:\n",
    "    gutenberg_stemmed.append(porter_stemmer.stem(token))\n",
    "\n",
    "gutenberg_stemmed[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "gutenberg_stemmed_tagged = nltk.pos_tag(gutenberg_stemmed)\n",
    "print(time.time() - start_time)\n",
    "#gutenberg_stemmed_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compare Stemming & Lemmatization\n",
    "print(porter_stemmer.stem('running'))\n",
    "print(lemma.lemmatize('running'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection library\n",
    "-  https://docs.python.org/3/library/collections.html#collections.Counter.most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections # token으로 나눠진 데이터를 딕셔너리 형태로 변환 + 편리한 함수를 제공하는 library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt # 결과를 시각화 하기 위한 matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# token - frequency 형태로 변환 : collections.Counter 함수!\n",
    "print(collections.Counter(gutenberg_stemmed_tagged).most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tuple 형태로 저장되어 있는 데이터를 token, frequency로 나눠서 저장\n",
    "token_list = []\n",
    "freq_list = []\n",
    "for token, freq in collections.Counter(gutenberg_stemmed_tagged).most_common(10):\n",
    "    token_list.append(token)\n",
    "    freq_list.append(freq)\n",
    "    \n",
    "print(token_list[:4])\n",
    "print(freq_list[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list로 나눈 데이터를 pandas 형태로 저장\n",
    "data = pd.concat([pd.DataFrame(token_list),pd.DataFrame(freq_list)], axis=1)\n",
    "data.columns = ['word','tag','freq']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word index 대신 word를 보여주는 그래프\n",
    "freqdist = nltk.FreqDist(gutenberg_lemma_tagged)\n",
    "freqdist.plot(50)\n",
    "freqdist.plot(50,cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 결과를 살펴보면 ','과 '.' 같이 단어가 아닌 문자가 높은 frequency를 가지고 있음\n",
    "#### Stop-words 제거 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nltk에서 제공되는 stop word 사용\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for index, ele in enumerate(stop_words):\n",
    "    if index<20:\n",
    "        print(index,ele)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# domain specific stop-words를 update하여 사용할 수 도 있음\n",
    "\n",
    "stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "\n",
    "# 대소문자가 다르게 인식되기 때문에 lowercase로 변환하여 사용\n",
    "filtered_words = [word[0].lower() for word in gutenberg_lemma_tagged if word[0].lower() not in stop_words]\n",
    "filtered_tag = [word[1].lower() for word in gutenberg_lemma_tagged if word[0].lower() not in stop_words]\n",
    "filtered_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stop word list에 'mr.' 추가하면 없어짐\n",
    "freqdist = nltk.FreqDist(filtered_words)\n",
    "freqdist.plot(50)\n",
    "freqdist.plot(50,cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 3 - 명사만 추출하여 고유한 리스트를 만들어 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataframe 설정\n",
    "result = pd.DataFrame()\n",
    "result['filtered_word'] = filtered_words\n",
    "result['filtered_freq'] = filtered_tag\n",
    "\n",
    "# csv 형태로 저장\n",
    "result.to_csv(\"filtered_word.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 4 - 추출한 리스트를 csv 형태로 저장 해보자"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
